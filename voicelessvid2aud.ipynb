{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc2AxyvPCT0E"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install opencv-python-headless pillow transformers gtts nltk sentence-transformers scikit-learn groq\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# Import libraries\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import PIL.Image\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import nltk\n",
        "from groq import Groq\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set environment variable for Groq API key\n",
        "os.environ['GROQ_API_KEY'] = 'Place_your_groq_Api_key_here'  # Replace with your actual Groq API key\n",
        "\n",
        "# Upload the video file\n",
        "uploaded = files.upload()\n",
        "video_path = next(iter(uploaded))\n",
        "\n",
        "# Create a directory to store frames\n",
        "if not os.path.exists('frames'):\n",
        "    os.makedirs('frames')\n",
        "\n",
        "# Calculate the total number of frames\n",
        "def get_total_frames(video_path):\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    video.release()\n",
        "    return total_frames\n",
        "\n",
        "# Extract frames at equal intervals\n",
        "def extract_frames(video_path, output_folder, percentage=30):\n",
        "    total_frames = get_total_frames(video_path)\n",
        "    selected_frames = int(total_frames * percentage / 100)\n",
        "    interval = total_frames / selected_frames\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    count = 0\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_count % int(interval) == 0:\n",
        "            frame_filename = os.path.join(output_folder, f\"frame_{count:04d}.jpg\")\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            count += 1\n",
        "        frame_count += 1\n",
        "    video.release()\n",
        "\n",
        "extract_frames(video_path, 'frames', percentage=30)\n",
        "\n",
        "# Load the BLIP model for image captioning\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    try:\n",
        "        image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "        out = model.generate(**inputs)\n",
        "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "        return caption\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def gather_captions(folder_path):\n",
        "    captions = []\n",
        "    for filename in sorted(os.listdir(folder_path)):\n",
        "        if filename.endswith('.jpg'):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            caption = generate_caption(image_path)\n",
        "            if caption:\n",
        "                captions.append(caption)\n",
        "    return captions\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess_captions(captions):\n",
        "    def preprocess_caption(caption):\n",
        "        caption = caption.lower()\n",
        "        tokens = word_tokenize(caption)\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    return [preprocess_caption(caption) for caption in captions]\n",
        "\n",
        "# Find the optimal number of clusters using silhouette score\n",
        "def find_optimal_clusters(embeddings):\n",
        "    possible_clusters = range(2, 21)  # Test from 2 to 20 clusters\n",
        "    best_n_clusters = 2\n",
        "    best_score = -1\n",
        "\n",
        "    for n_clusters in possible_clusters:\n",
        "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=0).fit(embeddings)\n",
        "        labels = kmeans.labels_\n",
        "        if len(set(labels)) > 1:  # Avoid silhouette score calculation with a single cluster\n",
        "            score = silhouette_score(embeddings, labels)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_n_clusters = n_clusters\n",
        "\n",
        "    return best_n_clusters\n",
        "\n",
        "# Remove redundant captions using optimized KMeans clustering\n",
        "def remove_redundancies(captions):\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(captions)\n",
        "    print(\"Caption Embeddings:\")\n",
        "    print(embeddings)  # Debug: Print embeddings\n",
        "\n",
        "    # Determine optimal number of clusters\n",
        "    optimal_clusters = find_optimal_clusters(embeddings)\n",
        "    print(f\"Optimal Number of Clusters: {optimal_clusters}\")\n",
        "\n",
        "    # Perform KMeans clustering with optimal number of clusters\n",
        "    kmeans = KMeans(n_clusters=optimal_clusters, n_init='auto', random_state=0).fit(embeddings)\n",
        "    labels = kmeans.labels_\n",
        "    print(\"KMeans Labels:\")\n",
        "    print(labels)  # Debug: Print KMeans labels\n",
        "\n",
        "    # Select one caption per cluster\n",
        "    unique_captions = []\n",
        "    for i in range(optimal_clusters):\n",
        "        cluster_indices = np.where(labels == i)[0]\n",
        "        if len(cluster_indices) > 0:\n",
        "            unique_captions.append(captions[cluster_indices[0]])\n",
        "\n",
        "    print(f\"Unique Captions after clustering: {unique_captions}\")\n",
        "    return unique_captions\n",
        "\n",
        "# Generate summary with Llama 8B using Groq API\n",
        "def generate_summary_with_llama(captions):\n",
        "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "    text = ' '.join(captions)\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Summarize the scenerio without redundancy, into minimum two lines/para: {text}\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "    )\n",
        "    summary = chat_completion.choices[0].message.content\n",
        "    return summary\n",
        "\n",
        "# Process captions and generate final summary\n",
        "captions = gather_captions('frames')\n",
        "\n",
        "# Display captions\n",
        "print(\"Frame Captions:\")\n",
        "for caption in captions:\n",
        "    print(caption)\n",
        "\n",
        "# Preprocess and remove redundancies\n",
        "preprocessed_captions = preprocess_captions(captions)\n",
        "print(\"Preprocessed Captions:\")\n",
        "print(preprocessed_captions)  # Debug: Print preprocessed captions\n",
        "\n",
        "unique_captions = remove_redundancies(preprocessed_captions)\n",
        "\n",
        "# Generate and display the final summary\n",
        "if unique_captions:\n",
        "    final_summary = generate_summary_with_llama(unique_captions)\n",
        "    print(f\"Total frames processed: {len(os.listdir('frames'))}\")\n",
        "    #print(\"Final Summary of the video:\")\n",
        "    print(final_summary)\n",
        "else:\n",
        "    print(\"No unique captions found.\")\n",
        "\n",
        "# Convert the final summary to speech\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text=text, lang='en', slow=False)\n",
        "    audio_file = '/content/output.mp3'\n",
        "    tts.save(audio_file)\n",
        "    return audio_file\n",
        "\n",
        "# Create and play the audio file\n",
        "audio_file = text_to_speech(final_summary)\n",
        "display(Audio(audio_file, autoplay=True))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
